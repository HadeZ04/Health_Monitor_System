#!/usr/bin/env python3
"""Script test cache performance cá»§a RAG retriever."""

import logging
import time
import sys
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

sys.path.insert(0, str(Path(__file__).parent))

from src.retriever import PubMedRetriever
from src.config import get_settings

def test_cache_performance():
    """Test cache performance vá»›i cÃ¡c queries khÃ¡c nhau."""
    print("=" * 70)
    print("TEST CACHE PERFORMANCE - RAG Retriever")
    print("=" * 70)
    
    settings = get_settings()
    retriever = PubMedRetriever(settings)
    
    if not retriever.available:
        print("âŒ RAG khÃ´ng available!")
        return
    
    print(f"\nâœ… RAG Ä‘Ã£ sáºµn sÃ ng")
    print(f"   - Cache size limit: {retriever._max_cache_size} files")
    print(f"   - Current cache: {len(retriever._file_cache)} files")
    
    # Danh sÃ¡ch cÃ¢u há»i test
    test_queries = [
        # NhÃ³m 1: CÃ¹ng chá»§ Ä‘á» (sáº½ hit cÃ¹ng files) - Test cache hit
        ("What is diabetes?", "Diabetes type 1"),
        ("How to treat diabetes?", "Diabetes management"),
        ("Diabetes symptoms and causes", "Diabetes complications"),
        
        # NhÃ³m 2: Chá»§ Ä‘á» khÃ¡c (cÃ³ thá»ƒ hit files khÃ¡c) - Test cache miss
        ("What is hypertension?", "Hypertension treatment"),
        ("Heart disease symptoms", "Cardiovascular disease"),
        
        # NhÃ³m 3: Quay láº¡i chá»§ Ä‘á» cÅ© (sáº½ hit cache) - Test cache reuse
        ("Diabetes medication", "Insulin therapy"),
        ("Type 2 diabetes", "Diabetes prevention"),
    ]
    
    print("\n" + "=" * 70)
    print("Báº®T Äáº¦U TEST CACHE")
    print("=" * 70)
    
    results = []
    
    for i, (query, description) in enumerate(test_queries, 1):
        print(f"\n[{i}/{len(test_queries)}] Query: '{query}'")
        print(f"   Description: {description}")
        
        # Äo thá»i gian
        start_time = time.time()
        docs = retriever.retrieve(query, top_k=5)
        elapsed = (time.time() - start_time) * 1000  # ms
        
        # Kiá»ƒm tra cache status
        cache_size = len(retriever._file_cache)
        
        print(f"   â±ï¸  Latency: {elapsed:.2f}ms")
        print(f"   ğŸ“„ Retrieved: {len(docs)} documents")
        print(f"   ğŸ’¾ Cache size: {cache_size} files")
        
        if docs:
            print(f"   ğŸ“Š Top score: {docs[0].get('score', 0):.4f}")
            # Hiá»ƒn thá»‹ files Ä‘Æ°á»£c truy cáº­p (tá»« indices)
            indices = [doc.get('rank', 0) for doc in docs]
            print(f"   ğŸ” Document indices: {indices[:3]}...")
        
        results.append({
            'query': query,
            'latency_ms': elapsed,
            'docs_count': len(docs),
            'cache_size': cache_size,
        })
        
        # Nghá»‰ má»™t chÃºt giá»¯a cÃ¡c queries
        time.sleep(0.1)
    
    # PhÃ¢n tÃ­ch káº¿t quáº£
    print("\n" + "=" * 70)
    print("PHÃ‚N TÃCH Káº¾T QUáº¢")
    print("=" * 70)
    
    # TÃ­nh trung bÃ¬nh latency
    latencies = [r['latency_ms'] for r in results]
    avg_latency = sum(latencies) / len(latencies)
    
    # PhÃ¢n loáº¡i: cold cache (3 queries Ä‘áº§u) vs warm cache (cÃ¡c query sau)
    cold_latencies = latencies[:3]
    warm_latencies = latencies[3:]
    
    print(f"\nğŸ“ˆ Thá»‘ng kÃª:")
    print(f"   - Tá»•ng queries: {len(results)}")
    print(f"   - Latency trung bÃ¬nh: {avg_latency:.2f}ms")
    print(f"   - Cold cache (3 queries Ä‘áº§u): {sum(cold_latencies)/len(cold_latencies):.2f}ms")
    if warm_latencies:
        print(f"   - Warm cache (cÃ¡c query sau): {sum(warm_latencies)/len(warm_latencies):.2f}ms")
        improvement = ((sum(cold_latencies)/len(cold_latencies) - sum(warm_latencies)/len(warm_latencies)) / 
                      (sum(cold_latencies)/len(cold_latencies))) * 100
        print(f"   - Cáº£i thiá»‡n: {improvement:.1f}%")
    
    print(f"\nğŸ’¾ Cache cuá»‘i cÃ¹ng:")
    print(f"   - Files trong cache: {len(retriever._file_cache)}")
    print(f"   - Cache limit: {retriever._max_cache_size}")
    
    # Test cache hit báº±ng cÃ¡ch query láº¡i
    print("\n" + "=" * 70)
    print("TEST CACHE HIT - Query láº¡i cÃ¢u há»i Ä‘áº§u tiÃªn")
    print("=" * 70)
    
    first_query = test_queries[0][0]
    print(f"\nQuery láº¡i: '{first_query}'")
    
    start_time = time.time()
    docs = retriever.retrieve(first_query, top_k=5)
    elapsed = (time.time() - start_time) * 1000
    
    print(f"   â±ï¸  Latency: {elapsed:.2f}ms")
    print(f"   ğŸ’¾ Cache size: {len(retriever._file_cache)} files")
    
    first_latency = results[0]['latency_ms']
    if elapsed < first_latency:
        improvement = ((first_latency - elapsed) / first_latency) * 100
        print(f"   âœ… Cache hit! Nhanh hÆ¡n {improvement:.1f}% so vá»›i láº§n Ä‘áº§u")
    else:
        print(f"   âš ï¸  Latency tÆ°Æ¡ng tá»± (cÃ³ thá»ƒ do overhead khÃ¡c)")
    
    print("\n" + "=" * 70)
    print("âœ… TEST HOÃ€N Táº¤T")
    print("=" * 70)

if __name__ == "__main__":
    test_cache_performance()

