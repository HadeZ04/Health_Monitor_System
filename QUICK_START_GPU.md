# üöÄ Quick Start - Ch·∫°y Qwen3 14B Tr√™n GPU

## ‚ö° T√≥m T·∫Øt Nhanh

### Y√™u C·∫ßu T·ªëi Thi·ªÉu:
- **GPU:** NVIDIA v·ªõi >= 16GB VRAM (RTX 4090, A5000, V100...)
- **CUDA:** >= 11.8
- **Python:** 3.10 ho·∫∑c 3.11

---

## üìù C√°c B∆∞·ªõc (5 Ph√∫t)

### 1Ô∏è‚É£ C√†i Dependencies
```bash
cd Health_Monitor_System

# C√†i PyTorch v·ªõi CUDA
pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# C√†i c√°c packages kh√°c
pip install -r requirements_gpu.txt
```

### 2Ô∏è‚É£ Ki·ªÉm Tra GPU
```bash
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"
```

**Expected Output:**
```
CUDA: True
GPU: NVIDIA GeForce RTX 4090
```

### 3Ô∏è‚É£ Ch·∫°y Server
```bash
# 8-bit quantization (KHUY·∫æN NGH·ªä - 16GB VRAM)
python qwen_router_server.py --port 8081 --quantize 8bit
```

**Ch·ªù ƒë·∫øn khi th·∫•y:**
```
‚úì Model loaded successfully!
üåê Server: http://0.0.0.0:8081
ü§ñ Mode: MODEL
```

### 4Ô∏è‚É£ Test (Terminal M·ªõi)
```bash
curl http://localhost:8081/health | python -m json.tool
```

**Expected Response:**
```json
{
  "status": "ok",
  "model": "Qwen/Qwen2.5-14B-Instruct",
  "mode": "model",
  "gpu": {
    "available": true,
    "device_name": "NVIDIA GeForce RTX 4090",
    "total_memory_gb": "24.00"
  }
}
```

### 5Ô∏è‚É£ Ch·∫°y Backend
```bash
# Terminal m·ªõi
cd backend
npm run dev
```

---

## üéÆ Ch·∫ø ƒê·ªô Quantization

| L·ªánh | VRAM | T·ªëc ƒê·ªô | Ch·∫•t L∆∞·ª£ng |
|------|------|--------|------------|
| `--quantize 4bit` | 10GB | Nhanh | T·ªët (90%) |
| `--quantize 8bit` | 16GB | Trung b√¨nh | R·∫•t t·ªët (95%) |
| Kh√¥ng flag | 28GB+ | Ch·∫≠m | Xu·∫•t s·∫Øc (100%) |

---

## üîß Troubleshooting

### ‚ùå "CUDA out of memory"
```bash
# Gi·∫£m xu·ªëng 4-bit
python qwen_router_server.py --port 8081 --quantize 4bit
```

### ‚ùå "torch.cuda.is_available() = False"
```bash
# Ki·ªÉm tra NVIDIA driver
nvidia-smi

# Reinstall PyTorch v·ªõi CUDA
pip uninstall torch
pip install torch==2.1.2 --index-url https://download.pytorch.org/whl/cu118
```

### ‚ùå Model download ch·∫≠m
```bash
# Set mirror (China)
export HF_ENDPOINT=https://hf-mirror.com

# Ho·∫∑c download tr∆∞·ªõc
huggingface-cli download Qwen/Qwen2.5-14B-Instruct
```

---

## üåê Ch·∫°y Remote GPU Server

### Tr√™n GPU Server:
```bash
python qwen_router_server.py --host 0.0.0.0 --port 8081 --quantize 8bit
```

### Tr√™n Backend Server:
```env
# backend/.env
QWEN_API_URL=http://192.168.1.100:8081  # Thay IP
```

---

## üìä Performance Benchmark

**RTX 4090 + 8-bit Quantization:**
- Load time: ~2 ph√∫t (l·∫ßn ƒë·∫ßu)
- Inference: ~180ms/request
- VRAM: ~16GB

**A100 + Full Precision:**
- Load time: ~3 ph√∫t
- Inference: ~150ms/request
- VRAM: ~28GB

---

## ‚úÖ Ho√†n Th√†nh!

Gi·ªù b·∫°n c√≥:
- ‚úÖ Qwen3 14B ch·∫°y tr√™n GPU
- ‚úÖ Backend k·∫øt n·ªëi v·ªõi Qwen
- ‚úÖ Ph√¢n lo·∫°i intent th√¥ng minh (kh√¥ng c√≤n mock)

**Xem th√™m:** [GPU_SETUP_GUIDE.md](GPU_SETUP_GUIDE.md) cho h∆∞·ªõng d·∫´n chi ti·∫øt
