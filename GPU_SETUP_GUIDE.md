# üéÆ H∆∞·ªõng D·∫´n Ch·∫°y Qwen3 14B Tr√™n GPU Server

## ‚úÖ **Y√™u C·∫ßu H·ªá Th·ªëng**

### Ph·∫ßn C·ª©ng:
- **GPU:** NVIDIA GPU v·ªõi CUDA support
- **VRAM:** 
  - 4-bit quantization: >= 10GB VRAM (RTX 3080, RTX 4070 Ti+)
  - 8-bit quantization: >= 16GB VRAM (RTX 4090, A5000, V100)
  - Full precision: >= 28GB VRAM (A100, H100)
- **RAM:** >= 16GB
- **Disk:** >= 50GB free space (ƒë·ªÉ t·∫£i model)

### Ph·∫ßn M·ªÅm:
- **OS:** Ubuntu 20.04+ / Windows 11 with WSL2
- **CUDA:** >= 11.8
- **cuDNN:** >= 8.6
- **Python:** 3.10 - 3.11
- **Driver:** Latest NVIDIA driver

---

## üì¶ **B∆∞·ªõc 1: C√†i ƒê·∫∑t CUDA & Dependencies**

### Tr√™n Ubuntu:
```bash
# C√†i CUDA Toolkit
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb
sudo dpkg -i cuda-keyring_1.1-1_all.deb
sudo apt-get update
sudo apt-get -y install cuda

# Ki·ªÉm tra CUDA
nvidia-smi
nvcc --version
```

### Tr√™n Windows (WSL2):
```powershell
# C√†i NVIDIA Driver cho Windows (host)
# Download t·ª´: https://www.nvidia.com/Download/index.aspx

# Trong WSL2, CUDA s·∫Ω t·ª± ƒë·ªông available
nvidia-smi
```

---

## üêç **B∆∞·ªõc 2: Setup Python Environment**

```bash
cd Health_Monitor_System

# T·∫°o virtual environment
python3.10 -m venv venv_gpu
source venv_gpu/bin/activate  # Linux/Mac
# venv_gpu\Scripts\activate  # Windows

# Upgrade pip
pip install --upgrade pip

# C√†i PyTorch v·ªõi CUDA support
pip install torch==2.1.2 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Ki·ªÉm tra CUDA trong PyTorch
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"

# C√†i c√°c dependencies kh√°c
pip install -r requirements_gpu.txt
```

---

## üöÄ **B∆∞·ªõc 3: Ch·∫°y Qwen3 14B Server**

### C√°ch 1: 8-bit Quantization (KHUY·∫æN NGH·ªä - 16GB VRAM)
```bash
python qwen_router_server.py --port 8081 --quantize 8bit
```

### C√°ch 2: 4-bit Quantization (10GB VRAM)
```bash
python qwen_router_server.py --port 8081 --quantize 4bit
```

### C√°ch 3: Full Precision (28GB+ VRAM)
```bash
python qwen_router_server.py --port 8081
```

### Ch·∫°y tr√™n Remote GPU Server:
```bash
# Cho ph√©p external access
python qwen3_router_server.py --host 0.0.0.0 --port 8081 --quantize 8bit
```

---

## üß™ **B∆∞·ªõc 4: Ki·ªÉm Tra Server**

### Test Health Endpoint:
```bash
# Local
curl http://localhost:8081/health | jq

# Remote
curl http://your-gpu-server-ip:8081/health | jq
```

**Expected Response:**
```json
{
  "status": "ok",
  "model": "Qwen/Qwen2.5-14B-Instruct",
  "mode": "model",
  "gpu": {
    "available": true,
    "device_name": "NVIDIA RTX 4090",
    "total_memory_gb": "24.00",
    "allocated_memory_gb": "15.23",
    "cached_memory_gb": "16.50"
  }
}
```

### Test Classification:
```bash
curl -X POST http://localhost:8081/classify \
  -H "Content-Type: application/json" \
  -d '{"input": "T√¥i ƒëau ng·ª±c d·ªØ d·ªôi"}'
```

---

## ‚öôÔ∏è **B∆∞·ªõc 5: C·∫•u H√¨nh Backend**

S·ª≠a file `backend/.env`:
```env
# N·∫øu GPU server c√πng m√°y v·ªõi backend
QWEN_API_URL=http://localhost:8081

# N·∫øu GPU server ·ªü m√°y kh√°c
QWEN_API_URL=http://192.168.1.100:8081  # Thay b·∫±ng IP GPU server
```

---

## üî• **Hi·ªáu Su·∫•t & T·ªëi ∆Øu**

### So S√°nh VRAM Usage:

| Quantization | VRAM Used | Latency | Quality |
|--------------|-----------|---------|---------|
| **4-bit**    | ~10GB     | 200ms   | 90%     |
| **8-bit**    | ~16GB     | 180ms   | 95%     |
| **FP16**     | ~28GB     | 150ms   | 100%    |

### Tips T·ªëi ∆Øu:
1. **Batch Size = 1** (chatbot th∆∞·ªùng x·ª≠ l√Ω 1 request/l·∫ßn)
2. **Temperature = 0.3** (·ªïn ƒë·ªãnh h∆°n cho classification)
3. **max_new_tokens = 512** (ƒë·ªß cho JSON response)

---

## üêõ **X·ª≠ L√Ω L·ªói**

### L·ªói: CUDA Out of Memory
```bash
# Gi·∫£m xu·ªëng quantization th·∫•p h∆°n
python qwen3_router_server.py --quantize 4bit

# Ho·∫∑c clear GPU cache
python -c "import torch; torch.cuda.empty_cache()"
```

### L·ªói: bitsandbytes not compiled with CUDA
```bash
# Reinstall bitsandbytes v·ªõi CUDA
pip uninstall bitsandbytes
pip install bitsandbytes --no-cache-dir
```

### L·ªói: Model download timeout
```bash
# Set HuggingFace cache
export HF_HOME=/path/to/large/storage
export HF_HUB_CACHE=/path/to/large/storage

# Ho·∫∑c download tr∆∞·ªõc
huggingface-cli download Qwen/Qwen2.5-14B-Instruct
```

---

## üìä **Monitoring GPU**

### Xem GPU Usage real-time:
```bash
watch -n 0.5 nvidia-smi
```

### Xem trong Python:
```python
import torch

print(f"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
print(f"Cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
```

---

## üö¶ **Ch·∫°y Nh∆∞ Service (Production)**

### Systemd Service (Linux):
```bash
sudo nano /etc/systemd/system/qwen-router.service
```

```ini
[Unit]
Description=Qwen3 Router Server
After=network.target

[Service]
Type=simple
User=your-user
WorkingDirectory=/path/to/Health_Monitor_System
Environment="PATH=/path/to/venv_gpu/bin"
ExecStart=/path/to/venv_gpu/bin/python qwen_router_server.py --port 8081 --quantize 8bit
Restart=always

[Install]
WantedBy=multi-user.target
```

```bash
sudo systemctl daemon-reload
sudo systemctl enable qwen-router
sudo systemctl start qwen-router
sudo systemctl status qwen-router
```

---

## üåê **Ch·∫°y T·ª´ Xa (Remote GPU Server)**

### Tr√™n GPU Server:
```bash
# M·ªü port firewall
sudo ufw allow 8081

# Ch·∫°y server
python qwen_router_server.py --host 0.0.0.0 --port 8081 --quantize 8bit
```

### Tr√™n Backend Server:
```env
# backend/.env
QWEN_API_URL=http://gpu-server-ip:8081
```

---

## üìà **Benchmark**

Tr√™n RTX 4090 (24GB VRAM):
- **Load time:** ~2-3 ph√∫t (l·∫ßn ƒë·∫ßu t·∫£i model)
- **Inference:** ~180ms/request
- **Throughput:** ~5 requests/second
- **VRAM usage:** ~16GB (8-bit)

---

**üéâ Xong! Gi·ªù b·∫°n c√≥ Qwen3 14B ch·∫°y tr√™n GPU server r·ªìi!**
